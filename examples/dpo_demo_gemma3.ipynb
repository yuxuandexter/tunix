{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q grain\n",
    "!pip install -q git+https://github.com/google/tunix\n",
    "!pip install -q git+https://github.com/google/qwix\n",
    "\n",
    "!pip uninstall -q -y flax\n",
    "!pip install -q git+https://github.com/google/flax.git\n",
    "\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import re\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import optax\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tunix.examples.data import translation_dataset as data_lib\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tunix.sft.dpo.dpo_trainer import DpoTrainingConfig\n",
    "from tunix.sft.dpo.dpo_trainer import DpoTrainer\n",
    "from tunix.sft.dpo.dpo_trainer import TrainingInput\n",
    "from huggingface_hub import snapshot_download\n",
    "from tunix.sft.dpo.dpo_trainer import _generate_ids_and_masks\n",
    "from tunix.models.gemma3 import model as gemma3_model_lib\n",
    "from datasets import concatenate_datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Data ======\n",
    "TRAIN_DATA_DIR = \"./data/train\"\n",
    "TEST_DATA_DIR = \"./data/test\"\n",
    "TRAIN_FRACTION = 1.0\n",
    "\n",
    "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
    "# ====== LoRA ======\n",
    "RANK = 8\n",
    "ALPHA = 16.0\n",
    "\n",
    "# ====== Sharding ======\n",
    "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "MAX_PROMPT_LENGTH = 192\n",
    "TOTAL_GENERATION_STEPS = 192\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "BETA = 0.1\n",
    "\n",
    "# === AdamW, warmup, cosine scheduler ===\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "\n",
    "# == Cosine decay with warmup scheduler ==\n",
    "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
    "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
    "# scheduler.\n",
    "BATCH_SIZE = 1\n",
    "NUM_BATCHES = 512\n",
    "NUM_TEST_BATCHES = 100\n",
    "EVAL_EVERY_N_STEPS = 100\n",
    "\n",
    "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
    "TRAIN_FRACTION = 1.0\n",
    "MAX_STEPS = int(NUM_BATCHES * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
    "# == Grad clipping ==\n",
    "# Grad clipping to prevent large gradients. Found this\n",
    "# important to keep KL divergence in check.\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# ====== Inference ======\n",
    "GENERATION_CONFIGS = {\n",
    "    # greedy search\n",
    "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    # some randomness\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    # liberal\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnxxLtLAv66G"
   },
   "source": [
    "# Load reference model and LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "ignore_patterns = [\n",
    "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
    "]\n",
    "print(f\"Downloading {model_id} from Hugging Face...\")\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
    ")\n",
    "print(f\"Model successfully downloaded to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  print(\"\\n--- TPU HBM Usage ---\")\n",
    "  for i, d in enumerate(jax.local_devices()):\n",
    "    stats = d.memory_stats()\n",
    "    used = stats.get(\"bytes_in_use\", 0)\n",
    "    limit = stats.get(\"bytes_limit\", 0)\n",
    "\n",
    "    hbm_used = stats.get(\"device:0:HBM0:bytes_in_use\", used)\n",
    "    hbm_limit = stats.get(\"device:0:HBM0:bytes_limit\", limit)\n",
    "\n",
    "    # Fallback if specific HBM stats not available\n",
    "    if hbm_limit == 0:\n",
    "      hbm_used = used\n",
    "      hbm_limit = limit\n",
    "\n",
    "    percentage = (hbm_used / hbm_limit * 100) if hbm_limit > 0 else 0\n",
    "\n",
    "    print(\n",
    "        f\"Device {i} ({d.device_kind}): Using {fmt_size(hbm_used)} /\"\n",
    "        f\" {fmt_size(hbm_limit)} ({percentage:.2f}%)\"\n",
    "    )\n",
    "\n",
    "  print(\"--- End HBM Usage ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CP_PATH = local_model_path\n",
    "\n",
    "model_config = (\n",
    "    gemma3_model_lib.Gemma3Config.gemma3_1b()\n",
    ")  # pick correponding config based on model version\n",
    "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
    "mesh = jax.make_mesh(*MESH)\n",
    "with mesh:\n",
    "  gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "      MODEL_CP_PATH, model_config, mesh\n",
    "  )\n",
    "  nnx.display(gemma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- HBM Usage AFTER Model Load ---\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gemma_tokenizer = data_lib.Gemma3Tokenizer()\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "#gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=gemma3,\n",
    "    tokenizer=gemma_tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "      #weight_qtype=\"nf4\",\n",
    "      #tile_size=4,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy model\n",
    "lora_gemma = get_lora_model(gemma3, mesh=mesh)\n",
    "nnx.display(lora_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNwb0GKuwFYb"
   },
   "source": [
    "Load evaluation data and evaluate the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\").select(range(NUM_TEST_BATCHES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
    "provide your reasoning. Place it between {reasoning_start} and \\\n",
    "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
    "value) between {solution_start} and {solution_end}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "def generate(\n",
    "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
    "):\n",
    "  \"\"\"Given prompt, generates text.\"\"\"\n",
    "\n",
    "  if isinstance(question, str):\n",
    "    input_batch = [\n",
    "        TEMPLATE.format(\n",
    "            system_prompt=SYSTEM_PROMPT,\n",
    "            question=question,\n",
    "        ),\n",
    "    ]\n",
    "  else:\n",
    "    input_batch = [\n",
    "        TEMPLATE.format(\n",
    "            system_prompt=SYSTEM_PROMPT,\n",
    "            question=q,\n",
    "        )\n",
    "        for q in question\n",
    "    ]\n",
    "\n",
    "  out_data = sampler(\n",
    "      input_strings=input_batch,\n",
    "      max_generation_steps=TOTAL_GENERATION_STEPS, # Was 768\n",
    "      temperature=temperature,\n",
    "      top_k=top_k,\n",
    "      top_p=top_p,\n",
    "      echo=False,\n",
    "      seed=seed if seed is not None else None,\n",
    "  )\n",
    "\n",
    "  output = out_data.text\n",
    "  if isinstance(question, str):\n",
    "    return output[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    f\"{reasoning_start}Let me\"\n",
    "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
    ")\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    sampler,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_passes=1,\n",
    "    corr_lst=False,\n",
    "    make_lst=False,\n",
    "):\n",
    "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
    "\n",
    "  response_lst = []\n",
    "  corr = 0\n",
    "  partially_corr = 0\n",
    "  corr_format = 0\n",
    "  total = 0\n",
    "\n",
    "  for batch in tqdm(dataset):\n",
    "    answers = batch[\"answer\"]\n",
    "    questions = batch[\"question\"]\n",
    "\n",
    "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
    "    for p in range(num_passes):\n",
    "      responses = generate(\n",
    "          questions, sampler, temperature, top_k, top_p, seed=p\n",
    "      )\n",
    "      for idx, response in enumerate(responses):\n",
    "        multiple_call_responses[idx].append(response)\n",
    "\n",
    "    for question, multiple_call_response, answer in zip(\n",
    "        questions, multiple_call_responses, answers\n",
    "    ):\n",
    "      # check answer\n",
    "      corr_ctr_per_question = 0\n",
    "      partially_corr_per_question = 0\n",
    "      corr_format_per_question = 0\n",
    "      for response in multiple_call_response:\n",
    "        extracted_response = (\n",
    "            guess.group(1)\n",
    "            if (guess := match_numbers.search(response)) is not None\n",
    "            else \"-1000000\"\n",
    "        )\n",
    "        try:\n",
    "          if float(extracted_response.strip()) == float(answer.strip()):\n",
    "            corr_ctr_per_question += 1\n",
    "\n",
    "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
    "          if ratio >= 0.9 and ratio <= 1.1:\n",
    "            partially_corr_per_question += 1\n",
    "        except:\n",
    "          print(\"SKIPPED\")\n",
    "\n",
    "        # check format\n",
    "        if match_format.search(response) is not None:\n",
    "          corr_format_per_question += 1\n",
    "\n",
    "        if (\n",
    "            corr_ctr_per_question > 0\n",
    "            and partially_corr_per_question > 0\n",
    "            and corr_format_per_question > 0\n",
    "        ):\n",
    "          break\n",
    "\n",
    "      if corr_ctr_per_question > 0:\n",
    "        corr += 1\n",
    "        if corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, multiple_call_response))\n",
    "      else:\n",
    "        if not corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, multiple_call_response))\n",
    "      if partially_corr_per_question > 0:\n",
    "        partially_corr += 1\n",
    "      if corr_format_per_question > 0:\n",
    "        corr_format += 1\n",
    "\n",
    "      total += 1\n",
    "      if total % 10 == 0:\n",
    "        print(\n",
    "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
    "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
    "        )\n",
    "\n",
    "  to_return = (\n",
    "      corr,\n",
    "      total,\n",
    "      corr / total * 100,\n",
    "      partially_corr / total * 100,\n",
    "      corr_format / total * 100,\n",
    "  )\n",
    "  if make_lst:\n",
    "    return to_return, response_lst\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "  if \"####\" not in text:\n",
    "    return None\n",
    "  return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
    "  # Download data\n",
    "  if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "  data = tfds.data_source(\n",
    "      \"gsm8k\",\n",
    "      split=split,\n",
    "      data_dir=data_dir,\n",
    "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "      download=True,\n",
    "  )\n",
    "\n",
    "  dataset = (\n",
    "      grain.MapDataset.source(data)\n",
    "      .shuffle(seed=42)\n",
    "      .map(\n",
    "          lambda x: {\n",
    "              # passed to model forward pass\n",
    "              \"prompts\": TEMPLATE.format(\n",
    "                  system_prompt=SYSTEM_PROMPT,\n",
    "                  question=x[\"question\"].decode(\"utf-8\"),\n",
    "              ),\n",
    "              # passed to reward functions\n",
    "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
    "              # passed to reward functions\n",
    "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
    "          }\n",
    "      )\n",
    "  )\n",
    "  return dataset\n",
    "\n",
    "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
    "    :NUM_TEST_BATCHES\n",
    "]\n",
    "\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corr, total, accuracy, partial_accuracy, format_accuracy), responses = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **GENERATION_CONFIGS[\"standard\"],\n",
    "    make_lst=True\n",
    ")\n",
    "print(\n",
    "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCAiINxUwYqx"
   },
   "source": [
    "# Load DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "gsm8k_train_dpo_dataset = dpo_dataset.filter(lambda x: x['in_gsm8k_train'])\n",
    "\n",
    "# Get the number of samples in the filtered dataset\n",
    "num_gsm8k_train_samples = len(gsm8k_train_dpo_dataset)\n",
    "print(f\"Number of samples with in_gsm8k_train=True: {num_gsm8k_train_samples}\")\n",
    "\n",
    "# Calculate how many more samples are needed\n",
    "total_samples_needed = NUM_BATCHES\n",
    "samples_to_add = total_samples_needed - num_gsm8k_train_samples\n",
    "print(f\"Number of additional random samples needed: {samples_to_add}\")\n",
    "\n",
    "if samples_to_add > 0:\n",
    "    # Randomly select additional samples from the original dataset\n",
    "    # Ensure we don't sample more than the total available in the original dataset\n",
    "    random_samples = dpo_dataset.shuffle(seed=42).select(range(min(samples_to_add, len(dpo_dataset))))\n",
    "    print(f\"Number of random samples selected: {len(random_samples)}\")\n",
    "\n",
    "    # Combine the filtered dataset and the random samples\n",
    "    combined_dpo_dataset = concatenate_datasets([gsm8k_train_dpo_dataset, random_samples])\n",
    "else:\n",
    "    combined_dpo_dataset = gsm8k_train_dpo_dataset\n",
    "\n",
    "print(f\"Total samples in the combined dataset: {len(combined_dpo_dataset)}\")\n",
    "\n",
    "# Display the combined dataset info\n",
    "print(\"\\nCombined Dataset Info:\")\n",
    "print(combined_dpo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dpo_dataset(dataset, tokenizer, max_prompt_length, total_generation_steps, batch_size):\n",
    "    processed_batches = []\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch_examples = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "        processed_examples = []\n",
    "        for example in batch_examples:\n",
    "            # Apply left padding to prompts\n",
    "            prompt_ids, prompt_mask = _generate_ids_and_masks(\n",
    "                [example[\"input\"]], tokenizer, max_prompt_length, left_pad=True\n",
    "            )\n",
    "\n",
    "            # Apply right padding to chosen and rejected responses\n",
    "            chosen_ids, chosen_mask = _generate_ids_and_masks(\n",
    "                [example[\"chosen\"]], tokenizer, total_generation_steps, left_pad=False\n",
    "            )\n",
    "            rejected_ids, rejected_mask = _generate_ids_and_masks(\n",
    "                [example[\"rejected\"]], tokenizer, total_generation_steps, left_pad=False\n",
    "            )\n",
    "            processed_examples.append({\n",
    "                \"prompt_ids\": prompt_ids.astype(np.int64),\n",
    "                \"prompt_mask\": prompt_mask.astype(np.float64), # Cast to float64\n",
    "                \"chosen_ids\": chosen_ids.astype(np.int64),\n",
    "                \"chosen_mask\": chosen_mask.astype(np.float64), # Cast to float64\n",
    "                \"rejected_ids\": rejected_ids.astype(np.int64),\n",
    "                \"rejected_mask\": rejected_mask.astype(np.float64), # Cast to float64\n",
    "            })\n",
    "\n",
    "        data_dict = {key: np.array([example[key] for example in processed_examples]) for key in processed_examples[0].keys()}\n",
    "\n",
    "\n",
    "        training_input = TrainingInput(\n",
    "            prompt_ids=data_dict[\"prompt_ids\"],\n",
    "            prompt_mask=data_dict[\"prompt_mask\"],\n",
    "            chosen_ids=data_dict[\"chosen_ids\"],\n",
    "            chosen_mask=data_dict[\"chosen_mask\"],\n",
    "            rejected_ids=data_dict[\"rejected_ids\"],\n",
    "            rejected_mask=data_dict[\"rejected_mask\"],\n",
    "        )\n",
    "        processed_batches.append(training_input)\n",
    "\n",
    "    return processed_batches\n",
    "\n",
    "processed_dpo_dataset = process_dpo_dataset(combined_dpo_dataset, gemma_tokenizer, MAX_PROMPT_LENGTH, TOTAL_GENERATION_STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySource():\n",
    "\n",
    "  def __init__(self, data):\n",
    "    self._data = data\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self._data[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._data)\n",
    "def _dummy_dataset(\n",
    "    source: MySource,\n",
    "    prompt_ids: np.ndarray,\n",
    "    prompt_mask: np.ndarray,\n",
    "    chosen_ids: np.ndarray,\n",
    "    chosen_mask: np.ndarray,\n",
    "    rejected_ids: np.ndarray,\n",
    "    rejected_mask: np.ndarray,\n",
    "):\n",
    "  return grain.MapDataset.source(source).map(\n",
    "      lambda x: TrainingInput(\n",
    "          prompt_ids=prompt_ids[x],\n",
    "          prompt_mask=prompt_mask[x],\n",
    "          chosen_ids=chosen_ids[x],\n",
    "          chosen_mask=chosen_mask[x],\n",
    "          rejected_ids=rejected_ids[x],\n",
    "          rejected_mask=rejected_mask[x],\n",
    "      )\n",
    "  )\n",
    "train_ds_dpo = _dummy_dataset(\n",
    "        range(len(processed_dpo_dataset)),\n",
    "        [processed_dpo_dataset[x].prompt_ids for x in range(len(processed_dpo_dataset))],\n",
    "        [processed_dpo_dataset[x].prompt_mask for x in range(len(processed_dpo_dataset))],\n",
    "        [processed_dpo_dataset[x].chosen_ids for x in range(len(processed_dpo_dataset))],\n",
    "        [processed_dpo_dataset[x].chosen_mask for x in range(len(processed_dpo_dataset))],\n",
    "        [processed_dpo_dataset[x].rejected_ids for x in range(len(processed_dpo_dataset))],\n",
    "        [processed_dpo_dataset[x].rejected_mask for x in range(len(processed_dpo_dataset))],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3SQTYTiwc-N"
   },
   "source": [
    "# Define optimizer and DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, learning rate scheduler, gradient clipping\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "if MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DPO Training (using previously defined config variables)\n",
    "dpo_config = DpoTrainingConfig(\n",
    "    beta=BETA,\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer = DpoTrainer(\n",
    "    model=lora_gemma,\n",
    "    ref_model=gemma3,\n",
    "    optimizer=optimizer,\n",
    "    training_config=dpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdRa0cQHwgVO"
   },
   "source": [
    "# Train and evaluate LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting DPO training...\")\n",
    "\n",
    "dpo_trainer.train(train_ds=processed_dpo_dataset)\n",
    "print(\"DPO training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corr, total, accuracy, partial_accuracy, format_accuracy), responses = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **GENERATION_CONFIGS[\"standard\"],\n",
    "    make_lst=True\n",
    ")\n",
    "print(\n",
    "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
