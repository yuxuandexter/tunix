{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b-whFvUXXMd"
      },
      "source": [
        "# Direct Preference Optimization (DPO) Training with Gemma3-1B\n",
        "\n",
        "This notebook demonstrates how to fine-tune a Gemma3-1B model using Direct Preference Optimization (DPO). DPO is a method for training language models to align with human preferences without requiring a separate reward model.\n",
        "\n",
        "## What this example covers:\n",
        "- Loading and setting up a pre-trained Gemma3-1B instruction-tuned model\n",
        "- Applying LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "- Processing DPO training data with prompt/chosen/rejected response pairs\n",
        "- Training the model using the DPO trainer from Tunix\n",
        "- Evaluating model performance on GSM8K mathematical reasoning tasks\n",
        "\n",
        "The training uses the Argilla DPO dataset containing preference pairs, focusing on GSM8K training examples to improve mathematical reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import jax\n",
        "import optax\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "import humanize\n",
        "from tunix.examples.data import translation_dataset as data_lib\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tunix.sft.dpo.dpo_trainer import DPOTrainingConfig\n",
        "from tunix.sft.dpo.dpo_trainer import DPOTrainer\n",
        "from huggingface_hub import snapshot_download\n",
        "from tunix.models.gemma3 import model as gemma3_model_lib\n",
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "from tunix.rl.utils import show_hbm_usage\n",
        "from tunix.sft import metrics_logger\n",
        "from orbax import checkpoint as ocp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparamters/Config\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
        "\n",
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "# ====== LoRA ======\n",
        "RANK = 32\n",
        "ALPHA = 16.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "\n",
        "MAX_PROMPT_LENGTH = 192\n",
        "MAX_RESPONSE_LENGTH = 192\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "BETA = 0.1\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "BATCH_SIZE = 1\n",
        "NUM_BATCHES = 512\n",
        "NUM_TEST_BATCHES = 100\n",
        "EVAL_EVERY_N_STEPS = 1024\n",
        "\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "TRAIN_FRACTION = 1.0\n",
        "MAX_STEPS = int(NUM_BATCHES * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnxxLtLAv66G"
      },
      "source": [
        "# Load reference model and LoRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33e6d559"
      },
      "source": [
        "## Reference Model and LoRA Model\n",
        "\n",
        "**Reference Model:** This is the original pre-trained Gemma3-1B instruction-tuned model that serves as the base for fine-tuning. It's loaded from the Hugging Face Hub.\n",
        "\n",
        "**LoRA Model:** This is a Low-Rank Adaptation of the reference model. LoRA is a parameter-efficient fine-tuning technique that injects small, trainable matrices into specific layers of the pre-trained model, significantly reducing the number of parameters that need to be updated during training. This makes fine-tuning much faster and requires less memory compared to fine-tuning the entire model. The LoRA model is built on top of the reference model, inheriting its pre-trained weights and capabilities, while allowing for efficient adaptation to the DPO task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ignore_patterns = [\n",
        "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
        "]\n",
        "print(f\"Downloading {model_id} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model successfully downloaded to: {local_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_CP_PATH = local_model_path\n",
        "\n",
        "model_config = (\n",
        "    gemma3_model_lib.Gemma3Config.gemma3_1b()\n",
        ")  # pick correponding config based on model version\n",
        "mesh = jax.make_mesh(*MESH)\n",
        "with mesh:\n",
        "  gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "      MODEL_CP_PATH, model_config, mesh\n",
        "  )\n",
        "  nnx.display(gemma3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- HBM Usage AFTER Model Load ---\")\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(GEMMA_TOKENIZER_PATH)\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=gemma3,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + MAX_RESPONSE_LENGTH + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Policy model\n",
        "lora_gemma = get_lora_model(gemma3, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNwb0GKuwFYb"
      },
      "source": [
        "Load evaluation data and evaluate the reference model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEMPLATE = \"\"\"\u003cstart_of_turn\u003euser\n",
        "{question}\u003cend_of_turn\u003e\n",
        "\u003cstart_of_turn\u003emodel\"\"\"\n",
        "\n",
        "\n",
        "def generate(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_generation_steps=MAX_RESPONSE_LENGTH,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      corr_ctr_per_question = 0\n",
        "\n",
        "      for response in multiple_call_response:\n",
        "        # Simple Accuracy: check for answer anywhere in the full response\n",
        "        try:\n",
        "          answer_no_comma = answer.replace(\",\", \"\")\n",
        "          response_no_comma = response.replace(\",\", \"\")\n",
        "          if (\n",
        "              answer.strip() in response.strip()\n",
        "              or answer_no_comma.strip() in response_no_comma.strip()\n",
        "          ):\n",
        "            corr_ctr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED accuracy check\")\n",
        "\n",
        "        if corr_ctr_per_question \u003e 0:\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question \u003e 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(f\"===\u003e {corr=}, {total=}, {corr / total * 100=}\")\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -\u003e str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -\u003e grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset\n",
        "\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "# After evaluating the reference model on the GSM8K test dataset, we achieved an accuracy of around 65%.\n",
        "\n",
        "(corr, total, accuracy), responses = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"standard\"],\n",
        "    make_lst=True,\n",
        "    num_passes=5,\n",
        ")\n",
        "print(f\"{corr=}, {total=}, {accuracy=}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a324dff"
      },
      "source": [
        "## DPO Dataset Preparation\n",
        "\n",
        "The DPO training dataset is loaded from the \"argilla/distilabel-intel-orca-dpo-pairs\" dataset on the Hugging Face Hub. This dataset contains preference pairs (chosen and rejected responses) for various prompts.\n",
        "\n",
        "To improve the model's performance on mathematical reasoning tasks, we prioritize samples from the GSM8K training set by filtering the dataset for records where `in_gsm8k_train` is True.\n",
        "\n",
        "Since the number of GSM8K training samples might be less than the desired `NUM_BATCHES` for training, we add a sufficient number of random samples from the rest of the dataset to reach the target batch size. This ensures we have enough data for training while giving more weight to the GSM8K examples, and also helps improve the model's performance on general use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dataset() -\u003e grain.MapDataset:\n",
        "  dpo_dataset = load_dataset(\n",
        "      \"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\"\n",
        "  )\n",
        "  gsm8k_train_dpo_dataset = dpo_dataset.filter(lambda x: x[\"in_gsm8k_train\"])\n",
        "\n",
        "  # Get the number of samples in the filtered dataset\n",
        "  num_gsm8k_train_samples = len(gsm8k_train_dpo_dataset)\n",
        "  print(\n",
        "      f\"Number of samples with in_gsm8k_train=True: {num_gsm8k_train_samples}\"\n",
        "  )\n",
        "\n",
        "  # Calculate how many more samples are needed\n",
        "  total_samples_needed = NUM_BATCHES * BATCH_SIZE\n",
        "  samples_to_add = total_samples_needed - num_gsm8k_train_samples\n",
        "  print(f\"Number of additional random samples needed: {samples_to_add}\")\n",
        "\n",
        "  if samples_to_add \u003e 0:\n",
        "    # Randomly select additional samples from the original dataset\n",
        "    # Ensure we don't sample more than the total available in the original dataset\n",
        "    random_samples = dpo_dataset.shuffle(seed=42).select(\n",
        "        range(min(samples_to_add, len(dpo_dataset)))\n",
        "    )\n",
        "    print(f\"Number of random samples selected: {len(random_samples)}\")\n",
        "\n",
        "    # Combine the filtered dataset and the random samples\n",
        "    combined_dpo_dataset = concatenate_datasets(\n",
        "        [gsm8k_train_dpo_dataset, random_samples]\n",
        "    )\n",
        "  else:\n",
        "    combined_dpo_dataset = gsm8k_train_dpo_dataset\n",
        "\n",
        "  print(f\"Total samples in the combined dataset: {len(combined_dpo_dataset)}\")\n",
        "\n",
        "  def _get_response(x):\n",
        "    for element in x:\n",
        "      if element[\"role\"] == \"assistant\":\n",
        "        return element[\"content\"]\n",
        "\n",
        "  dataset = grain.MapDataset.source(combined_dpo_dataset).map(\n",
        "      lambda x: {\n",
        "          \"prompts\": TEMPLATE.format(question=x[\"input\"]),\n",
        "          \"chosen_responses\": x[\"chosen\"],\n",
        "          \"rejected_responses\": x[\"rejected\"],\n",
        "      }\n",
        "  )\n",
        "  return dataset\n",
        "\n",
        "\n",
        "dataset = get_dataset().batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3SQTYTiwc-N"
      },
      "source": [
        "# Define optimizer and DPO Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/content/tmp/tensorboard/dpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/content/tmp/tensorboard/dpo --port=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure DPO Training\n",
        "dpo_config = DPOTrainingConfig(\n",
        "    beta=BETA,\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "    max_response_length=MAX_RESPONSE_LENGTH,\n",
        "    metrics_logging_options=metrics_logging_options,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        "    checkpointing_options=checkpointing_options,\n",
        ")\n",
        "\n",
        "dpo_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model=lora_gemma,\n",
        "    ref_model=gemma3,\n",
        "    optimizer=optimizer,\n",
        "    training_config=dpo_config,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdRa0cQHwgVO"
      },
      "source": [
        "# Train and evaluate LoRA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if mesh is None:\n",
        "  dpo_trainer.train(train_dataset)\n",
        "else:\n",
        "  with mesh:\n",
        "    dpo_trainer.train(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + MAX_RESPONSE_LENGTH + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "# After evaluating the finetuned model on the GSM8K test dataset, we achieved an accuracy of around 70%.\n",
        "\n",
        "(corr, total, accuracy), responses = evaluate(\n",
        "    test_dataset,\n",
        "    lora_sampler,\n",
        "    **GENERATION_CONFIGS[\"standard\"],\n",
        "    make_lst=True,\n",
        "    num_passes=5,\n",
        ")\n",
        "print(f\"{corr=}, {total=}, {accuracy=}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
