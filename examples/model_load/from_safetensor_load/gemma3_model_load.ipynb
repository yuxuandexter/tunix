{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP3WvcX4Vm2I"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/google/tunix\n",
        "\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "!pip uninstall -q -y flax\n",
        "!pip install --no-cache-dir git+https://github.com/google/flax.git\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q humanize"
      ]
    },
    {
      "metadata": {
        "id": "OSEz5DzpVs0O"
      },
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EOzZn4VwVsxR"
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from flax import nnx\n",
        "from huggingface_hub import snapshot_download\n",
        "import humanize\n",
        "import jax\n",
        "from tunix.models.gemma3 import model as model_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_lib"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RNqkatgHVsuD"
      },
      "cell_type": "code",
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  print(\"\\n--- TPU HBM Usage ---\")\n",
        "  for i, d in enumerate(jax.local_devices()):\n",
        "    stats = d.memory_stats()\n",
        "    used = stats.get(\"bytes_in_use\", 0)\n",
        "    limit = stats.get(\"bytes_limit\", 0)\n",
        "\n",
        "    hbm_used = stats.get(\"device:0:HBM0:bytes_in_use\", used)\n",
        "    hbm_limit = stats.get(\"device:0:HBM0:bytes_limit\", limit)\n",
        "\n",
        "    # Fallback if specific HBM stats not available\n",
        "    if hbm_limit == 0:\n",
        "      hbm_used = used\n",
        "      hbm_limit = limit\n",
        "\n",
        "    percentage = (hbm_used / hbm_limit * 100) if hbm_limit \u003e 0 else 0\n",
        "\n",
        "    print(\n",
        "        f\"Device {i} ({d.device_kind}): Using {fmt_size(hbm_used)} /\"\n",
        "        f\" {fmt_size(hbm_limit)} ({percentage:.2f}%)\"\n",
        "    )\n",
        "\n",
        "  print(\"--- End HBM Usage ---\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "COcS1YLBeBSx"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the weights and load into TPU"
      ]
    },
    {
      "metadata": {
        "id": "KWgpwuieVsqw"
      },
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-3-1b-it\"\n",
        "ignore_patterns = [\n",
        "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
        "]\n",
        "print(f\"Downloading {model_id} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model successfully downloaded to: {local_model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uob4MJ0MVsl0"
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
        "show_hbm_usage()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "E4BXD3O4VsfK"
      },
      "cell_type": "code",
      "source": [
        "MODEL_CP_PATH = local_model_path\n",
        "\n",
        "config = (\n",
        "    model_lib.Gemma3Config.gemma3_1b()\n",
        ")  # pick correponding config based on model version\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]  # update this based on your # TPU devices\n",
        "mesh = jax.make_mesh(*MESH)\n",
        "with mesh:\n",
        "  gemma3 = params_lib.create_model_from_safe_tensors(\n",
        "      MODEL_CP_PATH, config, mesh\n",
        "  )\n",
        "  nnx.display(gemma3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FN_F1x5jWKFR"
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n--- HBM Usage AFTER Model Load ---\")\n",
        "show_hbm_usage()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uL_mqGK9UUtc"
      },
      "cell_type": "markdown",
      "source": [
        "# Run inference"
      ]
    },
    {
      "metadata": {
        "id": "hOJU7xkZV8Gd"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CP_PATH)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_D2vm9oIUXWU"
      },
      "cell_type": "code",
      "source": [
        "from tunix.generate import sampler\n",
        "\n",
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Write a poem about Shakespeare\"},\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "sampler = sampler.Sampler(\n",
        "    gemma3,\n",
        "    tokenizer,\n",
        "    sampler.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=config.num_layers,\n",
        "        num_kv_heads=config.num_kv_heads,\n",
        "        head_dim=config.head_dim,\n",
        "    ),\n",
        ")\n",
        "out = sampler(inputs, max_generation_steps=128, echo=True)\n",
        "\n",
        "for t in out.text:\n",
        "  print(t)\n",
        "  print(\"*\" * 30)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
